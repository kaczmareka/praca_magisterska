Twitter sentiment dataset:
# on test data, ~balanced (?)
	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=False, output_number=False, calculate_overall_score=1, threshold=0.05, max_nodes=0
	nlp = spacy.load('en_core_web_sm')
	nlp.add_pipe("merge_noun_chunks")
	for i in range(test_tse_data.shape[0]):
		if i%200==0:
			print(i)
	text_to_check=test_tse_data['text'][i]
	test_tse_data['predicted_sentiment'][i]=graph_sentiment_analysis(text_to_check, calculate_overall_score=1, nlp=nlp)

	- accuracy, ba_tse: (0.6069609507640068, 0.6016598994840155)


	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=True, output_number=False, calculate_overall_score=1, threshold=0.05, max_nodes=0
	- accuracy_compound, ba_tse_compound: (0.6078098471986417, 0.5959738357743798)

	#here when I tried to delete non english, the library did not perform well (probably because the texts were too short to properly detect language) and the performance did not change
	

News sentiment dataset: 
# about 750 positive, 100 negative
# comments: there are also records not in english, so in one of the versions I delete them with `langdetect` library
	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=False, output_number=False, calculate_overall_score=1, threshold=0.0, max_nodes=0
	- acc, balanced_accuracy: (0.7570754716981132, 0.7626737967914439)

	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=True, output_number=False, calculate_overall_score=1, threshold=0.0, max_nodes=0
	- acc, ba: (0.7382075471698113, 0.7649732620320856)

	# parameters: text, nlp, lemmatization=False, max_distance=20, ner_list=[], compound=True, output_number=False, calculate_overall_score=1, threshold=0.0, max_nodes=0
	# non english deleted
	- acc, ba: (0.7458233890214797, 0.7801174228195389)